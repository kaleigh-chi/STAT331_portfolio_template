---
title: "STAT 331 Portfolio"
author: "Kaleigh Chi"
format: 
  html: 
    self-contained: true
layout: margin-left
editor: visual
execute: 
  eval: false
  echo: true
---

[**My Grade:**]{.underline} I believe my grade equivalent to course work evidenced below to be a B.

[**Learning Objective Evidence:**]{.underline} In the code chunks below, I provide code from a Lab or Challenge assignment where you believe you have demonstrated proficiency with the specified learning target. I also specified **where** the code came from (e.g., Lab 4 Question 2).

## Working with Data

**WD-1: I can import data from a *variety* of formats (e.g., csv, xlsx, txt, etc.).**

-   `csv`

```{r wd-1-csv}
#Lab 5, Revisiting Lab 2

library(tidyverse)
library(here)

surveys <- read_csv(here("STAT331", "Week 2", "Lab 2", "surveys.csv"))
```

-   `xlsx`

```{r wd-1-xlsx}
#PA 4, Data Import

library(readxl) 
library(tidyverse)


military <- read_xlsx(here::here("STAT331", 
                                 "Week 4", 
                                 "Practice Activity", 
                                 "gov_spending_per_capita.xlsx"), 
                      sheet = "Share of Govt. spending", 
                      skip = 7, 
                      n_max = 190)
```

-   `txt`

```{r wd-1-txt}
#PA 5.1: Regular Expressions, Setup

library(tidyverse)

message <- read_csv(here::here("STAT331", "Week 5", "PracticeActivity", "scrambled_message.txt"))
```

**WD-2: I can select necessary columns from a dataset.**

```{r wd-2}
#Challenge 4, joining data with avocado data set

zillow <- zillow %>%
  pivot_longer(cols = `1/31/2015`:`9/30/2017`,
               names_to = "date",
               values_to = "prices") %>%
  select(region, date, prices) %>%
  separate(date, c("month", "day", "year"), sep = "/") %>%
  mutate(
    year = as.integer(year),
    month = as.integer(month)
  ) %>%
  select(region, year, month, prices) %>%
  group_by(region, year, month) %>%
  summarise(meanHome = mean(prices))
```

**WD-3: I can filter rows from a dataframe for a *variety* of data types (e.g., numeric, integer, character, factor, date).**

-   numeric

```{r wd-3-numeric}
#Lab 3, Problem 11

hiphop_clean %>%
  filter(age < 20) %>%
  group_by(word) %>%
  summarise(Mean = mean(as.integer(familiarity))) %>%
  slice_max(Mean, n = 5)
```

-   character -- specifically a string

```{r wd-3-string}
#Lab 3, Problem 12

hiphop_clean %>%
  filter(ethnic_cat == "non-white", sex == "Female") %>%
  group_by(word) %>%
  summarise(Mean = mean(as.integer(familiarity))) %>%
  slice_max(Mean, n = 5)
```

-   factor

```{r wd-3-factor}
#Lab 3, Problem 14

hiphop_clean %>%
  filter(sex == "Male", 
         ethnic == "white", 
         16 < age, 
         age < 24, 
         word == "[to be] ghost",
         city > 9999,
         city < 60001,) %>%
  slice_max(bieber, n = 5)
```

-   date

```{r wd-3-date}
#PA 5: Dates and Time, Problem 1

suspects <- suspects %>%
  filter(pm(Time.Spotted))
```

**WD-4: I can modify existing variables and create new variables in a dataframe for a *variety* of data types (e.g., numeric, integer, character, factor, date).**

-   numeric

```{r wd-4-numeric}
#Challenge 4, joining data with avocado data set

zillow <- zillow %>%
  pivot_longer(cols = `1/31/2015`:`9/30/2017`,
               names_to = "date",
               values_to = "prices") %>%
  select(region, date, prices) %>%
  separate(date, c("month", "day", "year"), sep = "/") %>%
  mutate(
    year = as.integer(year),
    month = as.integer(month)
  ) %>%
  select(region, year, month, prices) %>%
  group_by(region, year, month) %>%
  summarise(meanHome = mean(prices))
```

-   character -- specifically a string

```{r wd-4-string}
#PA 5.1: Regular Expressions, Decode a Message - Problem 3

message <- message %>%
  mutate(
    Word = str_remove_all(Word, pattern = "ugh+h(\\?|\\!)"),
    Word = str_remove_all(Word, pattern = "ugh(\\?|\\!|\\.)")
  )
```

-   factor

```{r wd-4-factor}
#Lab 3, Problem 5

hiphop_clean <- hiphop |>
  mutate(
    across(c(jayz, jayz1, barkley, boondocks, boondocks1, monique, bieber), factor))
```

-   date

```{r wd-4-date}
#PA 5: Dates and Time, Problem 1

suspects <- suspects %>%
  mutate(Time.Spotted = ymd_hms(Time.Spotted),
         Time.Spotted = force_tz(Time.Spotted, 
                                 tzone = "America/Los_Angeles")
         )
```

**WD-5: I can use mutating joins to combine multiple dataframes.**

-   `left_join()`

```{r wd-5-left}
#N/A
```

-   `right_join()`

```{r wd-5-right}
#Preview Activity 4

right_join(prof_info, prof_course)
```

-   `inner_join()`

```{r wd-5-inner}
#Preview Activity 4

inner_join(prof_info, prof_course)
```

-   `full_join()`

```{r wd-5-full}
#Challenge 4, joining data with avocado data set

join_avocado <- full_join(zillow, 
                          avocado_cali, 
                          by = c("region", "month", "year")) %>%
  drop_na()
```

**WD-6: I can use filtering joins to filter rows from a dataframe.**

-   `semi_join()`

```{r wd-6-semi}
#Lab 4, Problem 5

avocado_clean <- semi_join(avocado, avocado_metro, by = "region") %>%
  filter(region == 'LosAngeles' | region == 'NewYork' | 
         region == "DallasFtWorth" | region == 'Houston' | 
         region == 'PhoenixTucson') %>%
  group_by(region) %>%
  mutate(mean_total_volume = mean(`Total Volume`)) 
```

-   `anti_join()`

```{r wd-6-anti}
#PA 4, Problem 4

military_clean <- military_clean |> 
  anti_join(cont_region, by = c("Country" = "Region"))
```

**WD-7: I can pivot dataframes from long to wide and visa versa**

-   `pivot_longer()`

```{r wd-7-long}
#Challenge 4, joining data with avocado data set

zillow <- zillow %>%
  pivot_longer(cols = `1/31/2015`:`9/30/2017`,
               names_to = "date",
               values_to = "prices") %>%
  select(region, date, prices) %>%
  separate(date, c("month", "day", "year"), sep = "/") %>%
  mutate(
    year = as.integer(year),
    month = as.integer(month)
  ) %>%
  select(region, year, month, prices) %>%
  group_by(region, year, month) %>%
  summarise(meanHome = mean(prices))
```

-   `pivot_wider()`

```{r wd-7-wide}
#Lab 4, Question 6

avocado_ca <- avocado_metro %>%
  filter(region == 'LosAngeles' | region == 'SanDiego' | region == 'Sacramento' | region == 'SanFrancisco') %>%
  group_by(type, region) %>%
  summarize(mean = mean(AveragePrice)) %>%
  pivot_wider(names_from = type, values_from = mean) %>%
  mutate(differences = abs(conventional - organic))
```

## Reproducibility

**R-1: I can create professional looking, reproducible analyses using RStudio projects, Quarto documents, and the here package.**

I've done this in the following provided assignments: Lab 2, , Challenge 3, Challenge 4

**R-2: I can write well documented and tidy code.**

-   Example 1

```{r r-2-1}
#Challenge 2

# globally groups distribution of weight by species, color codes distributions by genus, displays weight on the x axis, and displays species on the y axis
ggplot(data = surveys,
       mapping = aes(x = weight,
                     y = species,
                     group = species,
                     color = genus)) +
  # removes the outliers from displayed boxplot outputs
  geom_boxplot(outlier.shape = NA) + 
  # label the x and y axes accordingly
  labs(x = "Weight of animals (grams)", 
       y = "Species of animal") +
  # theme function to remove the legend from view
  theme(legend.position="none") +
  # gives the graph a gray background and white lines
  theme_gray() +
  # set the limit for x axis to display between 0 and 400 grams
  xlim(0, 400) +
  # annotate each species by genus and adjusted placement according to boxplots
  annotate("text", y = 1, x = 320, label = "Neotama") +
  annotate("text", y = 2, x = 125, label = "Chaetodipus") +
  annotate("text", y = 3, x = 110, label = "Peromyscus") +
  annotate("text", y = 4, x = 100, label = "Perognathus") +
  annotate("text", y = 5, x = 100, label = "Reithrodontomys") +
  annotate("text", y = 6, x = 180, label = "Sigmodon") +
  annotate("text", y = 7, x = 110, label = "Onychomys") +
  annotate("text", y = 8, x = 100, label = "Peromyscus") +
  annotate("text", y = 9, x = 100, label = "Reithrodontomys") +
  annotate("text", y = 10, x = 120, label = "Dipodomys") +
  annotate("text", y = 11, x = 130, label = "Dipodomys") +
  annotate("text", y = 12, x = 100, label = "Chaetodipus") +
  annotate("text", y = 13, x = 230, label = "Dipodomys") +
  annotate("text", y = 14, x = 80, label = "Onychomys")
```

-   Example 2

```{r r-2-2}
#Lab 4, Problem 7

ggplot(avocado_ca, aes(fill = size, 
                       x = region, 
                       y = mean)) +
  geom_bar(position = position_fill(reverse = TRUE), 
           stat = "identity") +
  scale_fill_manual("Avocado Size", 
                    values = c("lightgreen", "royalblue", "lightblue")) + 
  facet_grid(.~type) +
  labs(x = "Region of CA", 
       y = "Proportion of Mean Avocados Sold") +
  guides(fill = guide_legend(reverse = TRUE)) +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))
```

**R-3: I can write robust programs that are resistant to changes in inputs.**

-   Example 1

```{r r-3-1}
#Lab 5, Captures over the week - Question 1

rodents_captured <- surveys %>%
  filter(taxa == "Rodent") %>%
  select(day_of_week) %>%
  group_by(day_of_week) 

rodents_captured <- count(rodents_captured, 
                          'day_of_week') %>%
  drop_na()
```

-   Example 2

```{r r-3-2}
#Challenge 4, Joining Data with Avocado Data set

zillow <- zillow %>%
  pivot_longer(cols = `1/31/2015`:`9/30/2017`,
               names_to = "date",
               values_to = "prices") %>%
  select(region, date, prices) %>%
  separate(date, c("month", "day", "year"), sep = "/") %>%
  mutate(
    year = as.integer(year),
    month = as.integer(month)
  ) %>%
  select(region, year, month, prices) %>%
  group_by(region, year, month) %>%
  summarise(meanHome = mean(prices))
```

## Data Visualization & Summarization

**DVS-1: I can create visualizations for a *variety* of variable types (e.g., numeric, character, factor, date)**

-   numeric variables

```{r dvs-1-num}
#Lab 2, Problem 7

ggplot(data = surveys) +
  geom_jitter(mapping = aes(x = weight, 
                            y = hindfoot_length)) +
  facet_wrap(~ species)
```

-   numeric variables and categorical variables

```{r dvs-2-num-cat}
#Lab 5, Captures over the Week - Problem 3

ggplot(data = rodents_captured, 
       aes(x = week_end_day, 
           y = sum,
           fill = week_end_day)) +
  geom_bar(stat = "identity",
           show.legend = FALSE) +
  labs(x = "Day of Week", 
       title = "Total Number of Rodents Captured") +
  ylab(NULL) +
  ylim(0, 20000) +
  theme_light() +
  theme(plot.title.position = "plot")
```

-   categorical variables

```{r dvs-2-cat}
#Lab 3, Problem 10

ggplot(hiphop_clean) +
  geom_histogram(mapping = aes(y=ethnic, fill=sex),
                 stat = "count", 
                 position = "stack",
                 color = "black") +
  labs(title = "Hip Hop Study: Distribution of Racial Categories by Sex",
       y = "Racial Categories",
       x = "Frequency",
       fill = "Sex") +
  theme_light()
```

-   dates

```{r dvs-2-date}
#Lab 5, Time Series Plot - Problem 4

ggplot(data = mean_weight_vary, 
       aes(x = year, 
           y = mean_weight, 
           col = fct_rev(fct_reorder(genus,                       
                                     mean_weight)))) +
  geom_line() +
  scale_color_discrete(guide = guide_legend(title = "Genus")) +
  labs(x = "Year", 
       title = "Average Genus Weight Variation throughout Duration of Study") +
  ylab(NULL) +
  theme_light() +
  theme(plot.title.position = "plot")
```

**DVS-2: I use plot modifications to make my visualization clear to the reader.**

-   Example 1

```{r dvs-2-1}
#Lab 4, Problem 7

ggplot(avocado_ca, aes(fill = size, 
                       x = region, 
                       y = mean)) +
  geom_bar(position = position_fill(reverse = TRUE), 
           stat = "identity") +
  scale_fill_manual("Avocado Size", 
                    values = c("lightgreen", "royalblue", "lightblue")) + 
  facet_grid(.~type) +
  labs(x = "Region of CA", 
       y = "Proportion of Mean Avocados Sold") +
  guides(fill = guide_legend(reverse = TRUE)) +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))
```

-   Example 2

```{r dvs-2-2}
#Lab 4, Problem 5

ggplot(avocado_clean, aes(x = `Total Volume`, y  = region, fill = region)) +
  geom_boxplot() +
  scale_fill_manual(values = c("cadetblue", "darkseagreen", "lavender", "khaki", "paleturquoise")) +
  scale_x_continuous(labels = scales::comma) + 
  theme_light() + 
  labs(y = "Metro Region", 
       x = "Total Volume of Avocados Sold", 
       fill = "Metro Region", 
       title = "Avocados Sold by the Top 5 Avocado Selling Metro Regions in the US")
```

**DVS-3: I show creativity in my visualizations**

-   Example 1

```{r dvs-3-1}
#Lab 4, Problem 5

ggplot(avocado_clean, aes(x = `Total Volume`, y  = region, fill = region)) +
  geom_boxplot() +
  scale_fill_manual(values = c("cadetblue", "darkseagreen", "lavender", "khaki", "paleturquoise")) +
  scale_x_continuous(labels = scales::comma) + 
  theme_light() + 
  labs(y = "Metro Region", 
       x = "Total Volume of Avocados Sold", 
       fill = "Metro Region", 
       title = "Avocados Sold by the Top 5 Avocado Selling Metro Regions in the US")
```

-   Example 2

```{r dvs-3-2}
#Challenge 4, Avocado vs Housing Prices Plot

ggplot(data = join_avocado, aes(x = meanAvocado, y = meanHome)) +
  geom_point(color = "steelblue") +
  geom_smooth(method = "lm", se = TRUE, color = "orange") +
  facet_wrap(~region, ncol = 2) +
  labs(y = "Average Price of Home",
       x = "Average Price of Avocado",
       title = "Avocado Prices vs Homes in Major California Cities") +
  scale_x_continuous(labels = scales::dollar) +
  scale_y_continuous(labels = scales::dollar) 
```

**DVS-4: I can calculate numerical summaries of variables.**

-   Example 1

```{r dvs-4-1}
#Challenge 3, Comparing Male vs Female

hiphop_mean <- hiphop %>%
  group_by(sex) %>%
  summarize(across(intl:unclassifiable, ~mean(.x, na.rm = TRUE))) %>%
  pivot_longer(cols = "intl":"unclassifiable")
```

-   Example 2

```{r dvs-4-2}
#Challenge 3, Comparing White vs Non-White

hiphop_ethnic <- hiphop %>%
  group_by(ethnic_cat) %>%
  summarize(across(intl:unclassifiable,  ~mean(.x, na.rm = TRUE))) %>%
  pivot_longer(c("intl":"unclassifiable"))
```

**DVS-5: I can find summaries of variables across multiple groups.**

-   Example 1

```{r dvs-5-1}
#Challenge 3, Comparing Male vs Female

hiphop_mean <- hiphop %>%
  group_by(sex) %>%
  summarize(across(intl:unclassifiable, ~mean(.x, na.rm = TRUE))) %>%
  pivot_longer(cols = "intl":"unclassifiable")
```

-   Example 2

```{r dvs-5-2}
#Challenge 3, Comparing White vs Non-White

hiphop_ethnic <- hiphop %>%
  group_by(ethnic_cat) %>%
  summarize(across(intl:unclassifiable,  ~mean(.x, na.rm = TRUE))) %>%
  pivot_longer(c("intl":"unclassifiable"))
```

**DVS-6: I can create tables which make my summaries clear to the reader.**

-   Example 1 (needs to come from challenge 9)

```{r dvs-6-1}


```

-   Example 2

```{r dvs-6-2}


```

**DVS-7: I show creativity in my tables.**

-   Example 1 ( needs to come from challenge 9)

```{r dvs-7-1}

```

-   Example 2

```{r dvs-7-2}

```

## Program Efficiency

**PE-1: I can write concise code which does not repeat itself.**

-   using a single function call

```{r pe-1-one-call}
#Challenge 4, Joining Data with Avocado Data set

avocado_ca <- avocado %>%
  filter(region == 'LosAngeles' | region == 'SanDiego' | region == 'Sacramento' | region == 'SanFrancisco')
```

-   `across()`

```{r pe-1-across}
#Challenge 3, Comparing Male vs Female

hiphop_mean <- hiphop %>%
  group_by(sex) %>%
  summarize(across(intl:unclassifiable, ~mean(.x, na.rm = TRUE))) %>%
  pivot_longer(cols = "intl":"unclassifiable")
```

-   `map()` functions

```{r pe-1-map-1}
#Lab 2, Question 7

ggplot(data = surveys) +
  geom_jitter(mapping = aes(x = weight, 
                            y = hindfoot_length)) +
  facet_wrap(~ species)
```

**PE-2: I can write functions to reduce repetition in my code.**

-   Example 1

```{r pe2-1}
#Challenge 3, Comparing Male vs Female

hiphop_mean <- hiphop %>%
  group_by(sex) %>%
  summarize(across(intl:unclassifiable, ~mean(.x, na.rm = TRUE))) %>%
  pivot_longer(cols = "intl":"unclassifiable")
```

-   Example 2

```{r pe2-2}
#Challenge 3, Comparing White vs Non-White

hiphop_ethnic <- hiphop %>%
  group_by(ethnic_cat) %>%
  summarize(across(intl:unclassifiable,  ~mean(.x, na.rm = TRUE))) %>%
  pivot_longer(c("intl":"unclassifiable"))
```

**PE-3:I can use iteration to reduce repetition in my code.**

-   `across()`

```{r pe-3-across}
#Challenge 3, Comparing Male vs Female

hiphop_mean <- hiphop %>%
  group_by(sex) %>%
  summarize(across(intl:unclassifiable, ~mean(.x, na.rm = TRUE))) %>%
  pivot_longer(cols = "intl":"unclassifiable")
```

-   `map()` functions (Provide 2 Examples)

```{r pe-3-map-1}
#Lab 3, Question 10

ggplot(hiphop_clean) +
  geom_histogram(mapping = aes(y=ethnic, fill=sex),
                 stat = "count", 
                 position = "stack",
                 color = "black") +
  labs(title = "Hip Hop Study: Distribution of Racial Categories by Sex",
       y = "Racial Categories",
       x = "Frequency",
       fill = "Sex") +
  theme_light()
```

```{r pe-3-map-2}
#Lab 2, Question 7

ggplot(data = surveys) +
  geom_jitter(mapping = aes(x = weight, 
                            y = hindfoot_length)) +
  facet_wrap(~ species)
```

**PE-4: I can use modern tools when carrying out my analysis.**

-   Example 1

```{r pe-4-1}
#Lab 3, Question 11

hiphop_clean %>%
  filter(age < 20) %>%
  group_by(word) %>%
  summarise(Mean = mean(as.integer(familiarity))) %>%
  slice_max(Mean, n = 5)
hiphop_clean %>%
  filter(age < 20) %>%
  group_by(word) %>%
  summarise(Mean = mean(as.integer(familiarity))) %>%
  slice_min(Mean, n = 5)
```

-   Example 2

```{r pe-4-2}
#Challenge 3, Comparing Male vs Female

hiphop_mean <- hiphop %>%
  group_by(sex) %>%
  summarize(across(intl:unclassifiable, ~mean(.x, na.rm = TRUE))) %>%
  pivot_longer(cols = "intl":"unclassifiable")
```

## Data Simulation & Modeling

**DSM-1: I can simulate data from a *variety* of probability models.**

-   Example 1

```{r dsm-1-1}
# Practice Activity 9.2: Warmup

qunif(0.95, min = 1.5, max = 3.5)

qnorm(0.1, mean = 4.6, sd = 0.8)

pnorm(5, mean = 4.6, sd = 0.8, lower.tail = FALSE)

pchisq(5, df = 4, lower.tail = FALSE)

sum(rnorm(100, mean = 4.6, sd = 0.8) < 4)
```

-   Example 2

```{r dsm-1-2}
# Practice Activity 9.2: Catching a Con

set.seed(1957)

music_man <- function(n_tromb, n_cor, n_reed){
  
  trombones <- rnorm(n_tromb, mean = 4.6, sd = 0.8)
  cornets <- runif(n_cor, min = 1.5, max = 3.5)
  reeds <- rchisq(n_reed, df = 4)
  
  return(sum(trombones, cornets, reeds))
}

my_weights <- map_dbl(.x = 1:1000, 
                      .f = ~ music_man(n_tromb = 76, 
                                       n_cor = 110, 
                                       n_reed = 1035)
                      )

sum(my_weights < 4532)
```

**DSM-2: I can fit a linear regression and extract necessary summary measures.**

-   Example 1 ( these need to come from either practice activity 9.1 or lab 9)

```{r dsm-2-1}

```

-   Example 2

```{r dsm-2-2}

```

## Revising My Thinking

<!-- How did you revise your thinking throughout the course? How did you revise your thinking on the code examples you have provided in your portfolio? -->

I think I revised my thinking throughout this course by restructuring my approach to make code more user friendly. It's easy to make a code block run and carry out tasks, but it takes extra effort to make code more tidy by adding spaces and hitting enter after certain characters. Additionally, efficiency isn't something I was concerned about but condensing code to save a couple lines allows the computer to process the block faster. These were things I never really thought about that I've been growing on. When I move forward to my career or working with teams, I can't present messy or inefficient code because it's hard for people to read and makes work more inefficient.

Based on the code examples I've provided in my portfolio, I've revised my thinking by structuring my R code blocks cleanly and adding features, especially to my visualization, that makes my outputs more user friendly. When it comes to revising my thinking, I take into consideration the feedback given back to me from my peers and the feedback I am given from Dr. Theobold. My feedback from Dr. Theobold on Challenge 1 was to make the boxplots side by side. My issue was that I was using the old options style. From the revision of challenge 1, I have taken into consideration how to format options with the new options style and applied that to my subsequent activities. An instance of my takeaway from lab 1 can be seen in lab 3 problem 1 where I make use of the results option by calling #\|. If the Quarto file in my supporting_artifacts folder is opened, it can be noticed that my results are hidden so my readers don't have to see my packages being loaded.

Following lab 2, I learned that order mattered in the application of R especially in regards to ggplot and geom_point. A peer review came back suggesting that I make sure my boxplots aren't overlaying my points to help make the visualization more readable. I revised my lab 2 and took into account the order I overlay aesthetics for my subsequent plots. Challenge 4 is a graph I was especially proud of -and where I cited DVS-3 for "visualization creativity" -because I was able use many graphing techniques to capture the 4 major California cities while comparing avocado prices to prices of homes. I was careful of the order which I overlaid the different features so I tried intuitively adding on features as if someone was layering on features one after the other. Learning from lab 2 reviews, I was conscious about my challenge 4 visualization to viewers. I wanted them to see the correlation of avocado and home prices so I made those my axes. I even learned how to add a line of best fit to make the points make more sense and formatted dollar signs to my axes to make the prices visually digestible to viewers.

I completed a revision for lab 3 and challenge 3 but I only turned in challenge 3 because I wasn't sure how I could use my tokens to get a second revision. This was a minor understanding gap in the grading policies that I wished I understood sooner. I fixed lab 3 on my own by incorporating the advice I got from Dr. Theobold to make writing code more efficient with across() and hinting that I use slice functions to make my tables easier to look at. A peer commented that I made my code easy to follow and visually appealling which I was happy to hear. Although I could not revise lab 3, there is evidence I was able to nail the across() functions in the challenge 3 that received revisions. My challenge 3 is cited in DVS-5 for "finding summaries across multiple variables".

Continuing, my challenge 3 was revised by combining my male vs female and white vs non-white datasets into one table because I was originally unsure how I could group them. Although I received a success in challenge 3, I received a comment from Dr. Theobold that I could've used pivot_wider to get two different columns for female and male. I was able to make use of pivot_wider in Lab 4 where I cited it for proficiency in WD-7 for "pivoting dataframes".

Moving forward, I believe I need to be more timely with my revisions. After missing the first token revision chance, I now know how to submit my revisions so they can be officially looked at. It's been difficult for me to go to office hours with my outside commitments, but I've been able to communicate with Dr. Theobold on Discord which has been really great for me to ask questions. I also used one token on extending my lab 4. The extension has thrown me off in terms of staying on track with the rest of the class. I would really love a chance to revise as well so I'm making a commitment to myself to not ask for an extension again and keep up with my assignments the rest of the quarter.

## Extending My Thinking

<!-- How did you extended your thinking throughout the course? How did you extend your thinking on the code examples you have provided in your portfolio? -->

I extended my thinking throughout this course by doing my own research and outsourcing R tricks to make my visualizations and code more clean and efficient. I've created comments in the original Quarto files that cited my sources for the code examples I've provided in this portfolio.

I believe I have made attempts to extend my thinking. The first strong attempt I would like to note is Challenge 2. I had three choices to approach Challenge 2 and I went for the hottest challenge. Challenge 2 is something I awarded with R-2 for "well-documented and tidy code" based on the comments I incorporated, the structure and readability of my code, and it was a code block I was proud of due to my willingness to challenge myself with the "hottest" difficulty that week.

It is difficult to cite where I extended my thinking during Week 3 since the problems for the week were looking for specific answers. I'd like to point out Lab 3, Problem 10 where I made use of theme_light() to try spicing up my Lab 3 assignment for extending my thinking. I cited Lab 3, Problem 10 for DVS-1 for "creating visualizations for categorical variables".

When it came to lab 4, there were comments of sources I used every couple of problems that can be viewed in the Quarto. I believe my ability to outsource to the internet for help shows proof that I've extended my thinking. Additionally, it's the small aesthetics and extra features that I incorporate that shows my willingness to go above what is asked for in the problems. For example, in problem 5 of Lab 4, I made use of choosing my own colors for each region and used scale_x\_continuous to add comma signs. Lab 4 is an assignment where I cited proficiency in DVS-2 for being able to complete "plot modifications" and DVS-3 for being able to "show creativity in the visualizations". Challenge 4 was an assignment I was proud of as well, in particular, Challenge 4 got a DVS-3 for "showing creativity in visualization" and DSM-1 for "data simulation". It was learning about line of best fit through my own research, using lots of color, and changing the scale signs that added the extra feature that demonstrated extending my thinking.

Lab 5 is also an assignment I'd like to notably mention because I wasn't familiar with using forcats. Throughout Lab 5, there are comments of the sources where I learn about forcat features, editing graph features, and getting ideas from other's techniques on the Internet for how to approach Lab 5's problems. From Week 4's reflection, I told myself that I wanted to add more by doing something as small as changing the theme or throwing in color. There were a lot of graphs to experiment with in Lab 5. It can be seen in Weekly Captures - question 3, I made use of my forcats knowledge, used theme_light(), removed the legend, and changed the limits of the y-axis. This question in particular also demonstrated DVS-1 for "creating visualizations for numeric and categorical variables".

Moving forward, I think I'm doing well in terms of extending my thinking. I hope to myself that I keep up with adding something extra to my visualizations and expanding my R skills with outside sources with citations.

## Peer Support & Collaboration

<!-- Include an image of feedback you gave that you are proud of (either in a peer review or in Discord) -->

![This is a Peer Review that I am proud of. I put a lot of thought into this review in regard to how tidy and efficient their code was. I made sure to follow the rubric and cited specific spots where I thought they did well.](images/PeerReview.JPG){fig-align="center"}

The screenshot above is an example of a peer review I am proud of. I believe I have grown as a reviewer to give more thorough checks of my classmate's codes and even cite where in their code I am proud of or I believe needs improvement. When I started off this quarter I wasn't giving my feedback the most timely because I was up against the clock. But now I've been giving my reviews in a timely manner which I am proud of. I hope to maintain this for the rest of the quarter. I also believe I take my reviews seriously and it's important for me to provide feedback constructively where I can help someone improve without being discouraging.

In regards to group collaboration, I feel that I am always engaged with group work. My role thus far has always been the person supporting the drivers. I have my laptop on the side to search up anything the drivers need help on or read instructions. I have never overstepped my role of taking control of other's keyboards or seat. I am also committed to the assignments in class. An example of this is when my groupmate and I set up a Zoom meeting to finish Practice Activity 4 because we were not on campus during a time we were both free.

Something I do believe I lack is my interaction on Discord. I am the person who asks more questions than could answer. Or when there are the few questions I know, I am not online to answer first. I hope I can be more attentive to our classroom community finishing off this quarter.
